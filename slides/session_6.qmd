---
title: "Session 6: Special Topics and Best Practices"
subtitle: "Model Selection and Model Simplification"
---

## Overview

This final session addresses three important topics.

First, we cover model selection for mixed models with random effects.
Second, we clarify the distinction between prediction, explanation, and causation.
Third, we discuss transparent reporting practices for model selection studies.

These topics integrate concepts from previous sessions and address common challenges in empirical research.

---

## Model Selection with Mixed Models

Mixed models include both fixed effects and random effects.

The general form is:

$$y_{ij} = \mathbf{x}_{ij}'\boldsymbol{\beta} + \mathbf{z}_{ij}'\mathbf{u}_j + \varepsilon_{ij}$$

where $\boldsymbol{\beta}$ are fixed effects, $\mathbf{u}_j \sim N(0, \mathbf{G})$ are random effects for group $j$, and $\varepsilon_{ij} \sim N(0, \sigma^2)$.

Mixed models are common in longitudinal data, clustered data, and hierarchical designs.

Model selection must address both fixed and random effects.

---

## Estimating Mixed Models: ML vs. REML

Mixed models can be estimated using maximum likelihood or restricted maximum likelihood.

### ML (maximum likelihood)

ML estimates both fixed and random effects simultaneously.
The likelihood is:

$$L(\boldsymbol{\beta}, \mathbf{G}, \sigma^2 | \mathbf{y}) = p(\mathbf{y} | \boldsymbol{\beta}, \mathbf{G}, \sigma^2)$$

ML provides unbiased estimates of fixed effects but biased estimates of variance components.

### REML (restricted maximum likelihood)

REML integrates out the fixed effects and maximizes the likelihood of the residuals.
REML provides less biased estimates of variance components.
REML is the default in most software because variance estimation is often the primary interest.

---

## The REML Problem for Model Selection

### Issue

REML likelihoods are not comparable across models with different fixed effects.

The REML likelihood depends on the design matrix $\mathbf{X}$ for fixed effects.
Changing the fixed effects changes the REML likelihood even if fit is identical.

This makes AIC and BIC based on REML invalid for comparing fixed effects.

### Solution

When comparing models with different fixed effects, use ML estimation.
Fit all candidate models with ML, calculate AIC or BIC, and select the best model.
If desired, refit the final selected model with REML for inference on variance components.

When comparing models with different random effects but the same fixed effects, REML is valid.

---

## Selecting Fixed Effects in Mixed Models

### Procedure

Fit all candidate models with ML (not REML).
Calculate AIC or AICc for each model.
Rank models by AIC and compute Akaike weights.
Identify the 95% confidence set.

### Example

Compare models with different fixed effects but the same random structure:

- Model 1: $y_{ij} = \beta_0 + \beta_1 x_{1ij} + u_j + \varepsilon_{ij}$
- Model 2: $y_{ij} = \beta_0 + \beta_1 x_{1ij} + \beta_2 x_{2ij} + u_j + \varepsilon_{ij}$

Fit both with ML, compute AIC, and compare.

---

## Selecting Random Effects in Mixed Models

### The problem

Comparing random effects structures is more complex.

The null hypothesis that a variance component equals zero is on the boundary of the parameter space.
The asymptotic $\chi^2$ distribution for the LRT does not hold.

### Approaches

Use the likelihood ratio test with a boundary-corrected p-value: divide the LRT p-value by 2.
Use AIC or BIC, which do not require distributional assumptions.
Use model selection criteria designed for mixed models, such as conditional AIC.

### Practical advice

Start with a maximal random effects structure justified by the design.
Only simplify if there is strong evidence (large $\Delta \text{AIC}$) that simpler structure is adequate.
Do not use stepwise selection for random effects.

---

## Information Criteria for Mixed Models

### AIC for mixed models

$$\text{AIC} = -2\log L_{\text{ML}} + 2k$$

where $k$ is the total number of parameters, including variance components.

Variance components (e.g., $\sigma_u^2$, $\sigma^2$) each count as one parameter.

### Marginal vs. conditional AIC

Marginal AIC uses the marginal likelihood integrating over random effects.
Conditional AIC uses the likelihood conditional on estimated random effects.

In practice, standard AIC based on the marginal likelihood (the default in lme4, nlme) is appropriate for model comparison.

---

## Prediction, Explanation, and Causation

These are three distinct goals of modeling, each requiring different approaches to model selection.

### Prediction

Goal: Minimize out-of-sample prediction error.
Criterion: Cross-validated prediction accuracy, AIC.
Interpretation: Coefficients are not necessarily meaningful; focus is on accurate predictions.

### Explanation

Goal: Identify variables associated with the outcome.
Criterion: AICc, parsimony, interpretability.
Interpretation: Coefficients describe associations but not causal effects.

### Causation

Goal: Estimate causal effects of interventions.
Criterion: Subject-matter knowledge, causal diagrams, avoiding confounding bias.
Interpretation: Coefficients represent causal effects under assumptions.

---

## Model Selection for Prediction

When the goal is prediction, model selection should minimize expected prediction error.

### Appropriate methods

Use cross-validation to estimate out-of-sample error.
Use AIC or AICc to approximate prediction error.
Consider regularization (lasso, ridge, elastic net) to reduce overfitting.
Use model averaging to incorporate model uncertainty.

### What matters

Prediction accuracy on new data.
Avoiding overfitting.

### What does not matter

Interpretability of coefficients.
Whether predictors are "causal."
Significance of individual predictors.

If a predictor improves prediction, include it regardless of theoretical justification.

---

## Model Selection for Explanation

When the goal is explanation, model selection should identify a parsimonious set of variables that describe the outcome.

### Appropriate methods

Use AICc to balance fit and parsimony.
Consider BIC if the true model is believed to be among candidates.
Report all competitive models with $\Delta \text{AIC} < 2$.
Use model averaging to compute variable importance.

### What matters

Parsimony: a simple, interpretable model.
Empirical support: models with substantial Akaike weight.

### What does not matter

Whether associations are causal.

Explanation describes associations but does not establish causation.
Confounders may bias coefficient estimates even in a well-fitting model.

---

## Model Selection for Causation

When the goal is causal inference, model selection must address confounding.

### The fundamental problem

Causal inference requires controlling for confounders but not colliders or mediators.
Purely statistical model selection (AIC, stepwise) can select models that induce confounding bias.
A model with good predictive fit may have severely biased causal effect estimates.

### Appropriate methods

Use subject-matter knowledge and causal diagrams (DAGs) to specify models.
Include all confounders regardless of statistical significance.
Do not select variables based on AIC or p-values if the goal is causal inference.
Use sensitivity analyses to assess robustness to unmeasured confounding.

### What matters

Correct specification of confounders.
Avoiding selection of colliders or mediators.

Statistical fit (AIC, $R^2$) is not the primary criterion for causal models.

---

## Example: Confounding and Model Selection

Suppose we want to estimate the causal effect of exercise $(X)$ on heart disease $(Y)$.

Age $(Z)$ is a confounder: it affects both exercise and heart disease.
Body mass $(M)$ is a mediator: exercise affects BMI, which affects heart disease.

### Incorrect approach

Fit models with and without $Z$ and $M$, select based on AIC.
If $M$ is selected, the effect of $X$ is biased because we condition on a mediator.

### Correct approach

Include $Z$ (confounder) in the model.
Exclude $M$ (mediator) to estimate the total effect of $X$.
Do not use AIC to select between models; use causal reasoning.

This demonstrates that model selection for causation requires theory, not statistics alone.

---

## Predictive Modeling vs. Causal Modeling

### Predictive modeling

Goal: Forecast outcomes accurately.
Variables: Include anything that improves prediction.
Methods: AIC, cross-validation, regularization.
Example: Predict house prices for valuation.

### Causal modeling

Goal: Estimate effects of interventions.
Variables: Include confounders, exclude colliders and mediators.
Methods: Causal diagrams, sensitivity analysis.
Example: Estimate effect of exercise program on health outcomes.

### Key distinction

A model that predicts well may be a terrible causal model.
A model that estimates causal effects well may predict poorly.

Researchers must be clear about the goal and choose methods accordingly.

---

## Transparent Reporting of Model Selection

Transparent reporting is essential for reproducibility and to avoid misleading readers.

### What to report

The candidate set of models considered.
The model selection criterion used (e.g., AICc, cross-validation).
All models with $\Delta \text{AIC} < 2$ or in the 95% confidence set.
Whether the candidate set was pre-specified or data-driven.

### What not to do

Report only the "best" model without showing competitors.
Claim that the selected model is the "true" model.
Report p-values from a model selected based on statistical criteria without acknowledging selection bias.
Use stepwise selection and present the final model as if it were pre-specified.

---

## Example: Poor vs. Good Reporting

### Poor reporting

"We used stepwise regression to select the best model. The final model included predictors $A$, $B$, and $C$ (all $p < 0.05$)."

This does not disclose:
- How many models were considered.
- Which selection criterion was used.
- Whether other models had similar support.
- That p-values are invalid after selection.

### Good reporting

"We compared 12 candidate models using AICc. The best model included $A$, $B$, and $C$ ($\Delta \text{AICc} = 0$, weight = 0.48). Two additional models had $\Delta \text{AICc} < 2$ (see Table 1). We report model-averaged coefficients to account for model uncertainty. Standard errors reflect both parameter and model uncertainty."

This discloses the selection process and acknowledges uncertainty.

---

## Reporting Model Comparison Tables

A model comparison table should include:

- Model ID or description
- Number of parameters ($k$)
- Log-likelihood or deviance
- AIC or AICc
- $\Delta \text{AIC}$
- Akaike weight
- Cumulative weight (optional)

### Example table

| Model | k | Log-Lik | AICc | Î” AICc | Weight | Cumulative |
|-------|---|---------|------|--------|--------|------------|
| M3    | 5 | -120.3  | 250.9 | 0.0   | 0.52   | 0.52       |
| M2    | 4 | -121.5  | 251.2 | 0.3   | 0.45   | 0.97       |
| M4    | 6 | -120.1  | 252.5 | 1.6   | 0.23   | 1.00       |
| M1    | 3 | -125.8  | 257.7 | 6.8   | 0.02   | 1.00       |

Models M3, M2, and M4 are in the 95% confidence set.

---

## Pre-Registration and Model Selection

### The problem

Data-driven model selection inflates Type I error and produces overly optimistic results.
Post-hoc rationalization of selected models is common.

### Solutions

Pre-register the candidate set and selection strategy before data collection.
Specify which models will be compared and which criterion will be used.
If data-driven selection is unavoidable, acknowledge it and use validation data.

Pre-registration does not eliminate model uncertainty, but it reduces selective reporting and p-hacking.

---

## Cross-Validation for Validation

When model selection is data-driven, validate the selected model on independent data.

### Nested cross-validation

Outer loop: Split data into train and test sets.
Inner loop: Perform model selection on training data using cross-validation.
Fit selected model on full training data.
Evaluate on test data (which was not used for selection).

This provides an honest estimate of how the entire selection procedure performs on new data.

Simple cross-validation (without nesting) is invalid when used for both selection and evaluation.

---

## Reporting Uncertainty After Selection

### The problem

Standard errors and confidence intervals from a selected model are too narrow.
P-values are too small because they do not account for the search process.

### Solutions

Use data splitting: select on one subset, perform inference on another.
Use bootstrap or permutation methods to estimate uncertainty.
Use post-selection inference methods such as selective inference.
Report model-averaged estimates with unconditional standard errors.
Acknowledge that the model was data-selected and interpret cautiously.

### Practical advice

If you must select a model, be transparent about it.
Do not over-interpret p-values or confidence intervals from selected models.

---

## The Replication Crisis and Model Selection

Many published findings fail to replicate.

One major cause is data-driven model selection combined with selective reporting.

### The mechanism

Researchers fit many models.
They report the model with the best fit or most "interesting" results.
P-values and effect sizes are inflated due to selection.
Subsequent studies with independent data do not replicate the findings.

### The solution

Transparent reporting of all models considered.
Pre-registration of model selection strategy.
Validation on independent data.
Acknowledging model uncertainty through model averaging or confidence sets.

These practices improve the reliability and reproducibility of empirical research.

---

## Summary: Integrating Model Selection into Research

### For prediction

Use cross-validation and AIC to minimize prediction error.
Consider regularization for high-dimensional problems.
Use model averaging to improve prediction and quantify uncertainty.

### For explanation

Use AICc to identify parsimonious models with empirical support.
Report all competitive models, not just the "winner."
Use model-averaged coefficients and variable importance.

### For causation

Use subject-matter knowledge and causal diagrams to specify models.
Do not rely on AIC or statistical selection.
Include confounders; avoid conditioning on mediators or colliders.

### For transparency

Report the candidate set, selection criterion, and all competitive models.
Acknowledge when selection is data-driven.
Validate selected models on independent data when possible.

---

## Looking Ahead

### This workshop

We covered six sessions on model selection and model simplification.

Session 1 introduced the bias-variance tradeoff and the overfitting problem.
Session 2 covered cross-validation and information criteria for estimating out-of-sample error.
Session 3 addressed frameworks for comparing models, including likelihood ratio tests and $\Delta \text{AIC}$.
Session 4 examined variable selection methods, including stepwise, all-subsets, and regularization.
Session 5 introduced model averaging as an alternative to single-model selection.
Session 6 addressed mixed models, prediction versus causation, and transparent reporting.

### Key principles

Out-of-sample prediction error is the target quantity for model comparison.
Training error always decreases with complexity; it is a poor guide for model selection.
Cross-validation and information criteria provide better estimates of out-of-sample performance.
Model averaging acknowledges uncertainty when multiple models have similar support.
The goal of modeling (prediction, explanation, causation) determines the appropriate selection method.
Transparent reporting of model selection is essential for reproducibility.

---

## Final Recommendations

Use theory and prior knowledge to define a small candidate set.
Do not fit all possible models or use stepwise selection for final inference.
Report all models with $\Delta \text{AIC} < 2$, not just the "best" one.
Use model averaging when multiple models are competitive.
For causal inference, rely on subject-matter knowledge rather than statistical selection.
Validate selected models on independent data when possible.
Be transparent about the model selection process in publications.
Pre-register model selection strategies to reduce selective reporting.

Following these principles will improve the rigor, transparency, and reproducibility of empirical research using statistical models.
